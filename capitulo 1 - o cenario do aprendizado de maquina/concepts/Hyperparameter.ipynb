{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce91b2f-4f5e-4345-9f26-02210f0e3f3f",
   "metadata": {},
   "source": [
    "# Hyperparameter\n",
    "\n",
    "O ajuste de hiperparâmetros, também conhecido como otimização de hiperparâmetros, é o processo de encontrar os melhores hiperparâmetros para um modelo de machine learning para atingir o desempenho ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f09214-4566-4beb-8db3-9b078a3ca15e",
   "metadata": {},
   "source": [
    "## Examples of hyperparameters include:\n",
    "\r",
    "1. Learning Rate: A taxa de aprendizado define o tamanho do passo ao atualizar os parâmetros do modelo durante o treinamento. Pode influenciar tanto a velocidade de convergência quanto a precisão final do modelo.\n",
    "\n",
    "2. Number of Trees (para métodos de ensemble): Em algoritmos como Random Forest e Gradient Boosting, o número de árvores no ensemble é um hiperparâmetro fundamental.\n",
    "\n",
    "3. Kernel Choice (para Support Vector Machines): Em SVMs, a escolha do kernel (por exemplo, linear, polinomial ou função de base radial) é um hiperparâmetro que influencia o formato da fronteira de decisão.\n",
    "\n",
    "4. Depth of Trees (para Decision Trees): A profundidade máxima das árvores de decisão controla a complexidade das árvores e sua capacidade de ajustar aos dados de treinamento.ng data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f49593b-6e49-4404-a422-986b19ca4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36c39c8-3ac6-48c5-8f4c-fd827c409397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1e4f1-a457-4413-8c67-f516d0b692da",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Grid Search é uma técnica simples e de força bruta. Ela envolve especificar uma lista de valores de hiperparâmetros para explorar exaustivamente. Ela cria uma grade de todas as combinações possíveis de hiperparâmetros e avalia o modelo para cada combinação para encontrar a melhor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9cb0cd6-3830-42e3-950b-dd430097d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "540 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "158 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "382 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.93333333 0.94166667 0.95\n",
      " 0.95       0.95       0.95       0.95       0.95       0.95\n",
      " 0.93333333 0.94166667 0.95       0.95       0.95       0.94166667\n",
      " 0.95       0.95       0.94166667 0.95       0.94166667 0.94166667\n",
      " 0.95       0.95833333 0.95       0.95833333 0.95       0.94166667\n",
      " 0.95       0.93333333 0.94166667 0.95       0.94166667 0.95\n",
      " 0.95       0.95       0.95       0.95       0.94166667 0.94166667\n",
      " 0.95       0.95       0.95833333 0.95       0.94166667 0.95833333\n",
      " 0.95833333 0.95       0.95       0.94166667 0.93333333 0.95\n",
      " 0.95       0.95       0.95833333        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95       0.95       0.95       0.93333333 0.95       0.95\n",
      " 0.95833333 0.95       0.95833333 0.95833333 0.95833333 0.95\n",
      " 0.95       0.95       0.94166667 0.95       0.96666667 0.95\n",
      " 0.95       0.95833333 0.95833333 0.94166667 0.95       0.94166667\n",
      " 0.95833333 0.95       0.95833333 0.95       0.94166667 0.95\n",
      " 0.95       0.95       0.95       0.95       0.95       0.95\n",
      " 0.95       0.93333333 0.94166667 0.95       0.95       0.95\n",
      " 0.94166667 0.95833333 0.95833333 0.95       0.94166667 0.95833333\n",
      " 0.94166667 0.94166667 0.95833333 0.94166667 0.95       0.95833333\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.93333333 0.95       0.95\n",
      " 0.95       0.95       0.95       0.95833333 0.95833333 0.95\n",
      " 0.94166667 0.95       0.94166667 0.95       0.94166667 0.95833333\n",
      " 0.95       0.96666667 0.95       0.94166667 0.94166667 0.95\n",
      " 0.95       0.94166667 0.94166667 0.93333333 0.95       0.95833333\n",
      " 0.94166667 0.94166667 0.94166667 0.95       0.94166667 0.95833333\n",
      " 0.95       0.95       0.95       0.94166667 0.94166667 0.95833333\n",
      " 0.95       0.94166667 0.95833333 0.95       0.95       0.95\n",
      " 0.95       0.94166667 0.96666667 0.96666667 0.94166667 0.94166667\n",
      " 0.95       0.95       0.95              nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95       0.95       0.95       0.95       0.95       0.95\n",
      " 0.95833333 0.95       0.95       0.95       0.94166667 0.95\n",
      " 0.95       0.95       0.95       0.95       0.95       0.95833333\n",
      " 0.95       0.95       0.95833333 0.94166667 0.95       0.94166667\n",
      " 0.94166667 0.94166667 0.95833333 0.95833333 0.94166667 0.95\n",
      " 0.95       0.94166667 0.95       0.94166667 0.95       0.95\n",
      " 0.94166667 0.95833333 0.95       0.94166667 0.94166667 0.95\n",
      " 0.95       0.94166667 0.95       0.94166667 0.95       0.95833333\n",
      " 0.95       0.95       0.95       0.95833333 0.95       0.95      ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters - Grid Search: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Grid Search Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Grid Search\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Hyperparameters - Grid Search:\", grid_search.best_params_)\n",
    "\n",
    "# avaliar modelos com melhores hiperparametros\n",
    "grid_search_best_model = grid_search.best_estimator_\n",
    "grid_search_accuracy = grid_search_best_model.score(X_test, y_test)\n",
    "print(\"Grid Search Test Accuracy:\", grid_search_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d78e32-6e85-4d65-bc51-042a886b3002",
   "metadata": {},
   "source": [
    "### Best Hyperparameters - Grid Search: \n",
    "{'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100}\r\n",
    "Grid Search Test Accuracy: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f91604-e9bd-4df2-b468-5a32f27e168e",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "\n",
    "Seleciona aleatoriamente um conjunto de valores de hiperparâmetros para avaliação. Esse método pode frequentemente encontrar bons hiperparâmetros mais rápido porque explora um intervalo mais amplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9942be-85c5-4202-b76b-d51824b1be81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "20 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.94166667 0.94166667 0.95833333 0.95833333 0.95\n",
      " 0.95              nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters - Random Search: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 20}\n",
      "Random Search Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Busca aleatória\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_grid, n_iter=10, cv=5, n_jobs=-1) \n",
    "random_search.fit(X_train, y_train) \n",
    "\n",
    "# Imprime os melhores hiperparâmetros encontrados por cada método\n",
    "print( \"Best Hyperparameters - Random Search:\" , random_search.best_params_) \n",
    "\n",
    "# Avalia os modelos com os melhores hiperparâmetros no conjunto de testes\n",
    "random_search_best_model = random_search.best_estimator_ \n",
    "random_search_accuracy = random_search_best_model.score(X_test, y_test) \n",
    "print( \"Random Search Test Accuracy:\" , random_search_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9238a8a-79f2-4da2-b4f0-8ffef48f38ce",
   "metadata": {},
   "source": [
    "## Best Hyperparameters - Random Search: \n",
    "{'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 20}\r\n",
    "Random Search Test Accuracy: 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071a8c3-e164-4db3-b83b-209641cb3358",
   "metadata": {},
   "source": [
    "## Bayesian Optimization \n",
    "Usa modelos probabilísticos para modelar a função objetiva (como precisão do modelo) e decide onde avaliar em seguida com base nas previsões do modelo. Ela tende a selecionar hiperparâmetros que provavelmente melhorarão o modelo, tornando-o altamente eficiente em encontrar boas configurações em menos iterações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62401509-e9a6-4cc0-a9c8-c3d5bd40b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-optimize\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707c5826-4474-4dc1-8c61-6dc0e5b39aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hiperparametros\n",
    "\n",
    "param_space = {\n",
    "    'n_estimators': (50, 200),\n",
    "    'max_depth': (1, 20),\n",
    "    'min_samples_split': (2, 10),\n",
    "    'min_samples_leaf': (1, 4),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbdba741-21f5-4dca-8822-38d46db20a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Hyperparameters - Bayesian Optimization: OrderedDict({'max_depth': 9, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50})\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "opt = BayesSearchCV(\n",
    "    clf, param_space, cv=5, n_iter=50, n_jobs=-1, random_state=42, verbose=1\n",
    ")\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters - Bayesian Optimization:\", opt.best_params_)\n",
    "\n",
    "best_model = opt.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c04dbe7-974a-4fe8-a461-5713ce64319c",
   "metadata": {},
   "source": [
    "## Best Hyperparameters - Bayesian Optimization: \n",
    "OrderedDict({'max_depth': 9, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50})\r\n",
    "Test Accuracy: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a4544e-1374-4229-80d7-be65332f933f",
   "metadata": {},
   "source": [
    "## Resumo\n",
    "\n",
    "- A Pesquisa em Grade é simples e confiável, mas pode ser computacionalmente cara para grandes espaços de pesquisa.\r\n",
    "\r\n",
    "- A busca aleatória é mais eficiente para espaços de busca maiores e geralmente encontra boas soluções rapidamente.\r\n",
    "\r\n",
    "- A otimização bayesiana é eficiente para espaços de busca complexos e requer menos iterações do que a busca em grade ou aleatória."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
