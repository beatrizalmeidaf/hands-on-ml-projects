{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a5969a",
   "metadata": {},
   "source": [
    "# Visualização com TensorBoard e Ajuste de Hiperparâmetros \n",
    "\n",
    "## TensorBoard: Visualizando o Treinamento\n",
    "\n",
    "**TensorBoard** é uma ferramenta visual interativa que permite acompanhar o treinamento de redes neurais em tempo real. Com ele, você pode:\n",
    "\n",
    "* Visualizar curvas de aprendizado (perda e métricas)\n",
    "* Comparar execuções diferentes do modelo\n",
    "* Inspecionar o grafo computacional\n",
    "* Ver projeções de dados em alta dimensão\n",
    "* Monitorar imagens, histogramas e outros dados (dependendo da configuração)\n",
    "\n",
    "\n",
    "\n",
    "## Ajustando Hiperparâmetros \n",
    "\n",
    "Redes neurais têm muitos hiperparâmetros: número de camadas, neurônios, taxa de aprendizado, função de ativação etc. Para descobrir os melhores valores, usamos buscas como:\n",
    "\n",
    "* **Grid Search**: testa combinações exaustivamente\n",
    "* **Randomized Search**: testa combinações aleatórias dentro de um intervalo\n",
    "\n",
    "\n",
    "### Número de Camadas Ocultas (Hidden Layers)\n",
    "\n",
    "* **Ponto de partida**: Uma única camada oculta pode ser suficiente para muitos problemas simples.\n",
    "* **Teorema Universal da Aproximação**: Uma rede com apenas uma camada oculta e neurônios suficientes pode aproximar qualquer função contínua.\n",
    "* **Por que redes profundas são melhores?**\n",
    "\n",
    "  * Elas são **mais eficientes em termos de parâmetros**: podem representar funções complexas com menos neurônios.\n",
    "  * Capturam **estruturas hierárquicas** nos dados, como:\n",
    "\n",
    "    * Camadas inferiores → padrões simples (ex: linhas)\n",
    "    * Camadas intermediárias → formas (ex: quadrados, círculos)\n",
    "    * Camadas superiores → objetos completos (ex: rostos)\n",
    "* **Exemplo intuitivo**: desenhar uma floresta sem copiar/colar folhas → redes profundas fazem \"copy/cola\" estrutural.\n",
    "* **Benefícios**:\n",
    "\n",
    "  * Convergência mais rápida\n",
    "  * Melhor generalização\n",
    "  * Possibilidade de **Transfer Learning**\n",
    "\n",
    "### Transfer Learning (Aprendizado por Transferência)\n",
    "\n",
    "* Reutiliza as **camadas inferiores** de uma rede treinada em uma nova tarefa semelhante.\n",
    "* Exemplo:\n",
    "\n",
    "  * Rede pré-treinada para detectar rostos → camadas inferiores aprendem formas básicas.\n",
    "  * Nova rede para detectar penteados → pode reaproveitar essas camadas e treinar só as superiores.\n",
    "\n",
    "\n",
    "### Número de Neurônios por Camada Oculta\n",
    "\n",
    "* **Entrada/Saída**: Definidos pela natureza do problema (ex: MNIST → 784 entradas e 10 saídas).\n",
    "* **Prática antiga**: Arquitetura piramidal (300 → 200 → 100).\n",
    "* **Prática atual**: Manter o mesmo número de neurônios em todas as camadas pode funcionar melhor.\n",
    "* **Dica**:\n",
    "\n",
    "  * Aumente o número de neurônios **ou** camadas até começar a **overfitting**.\n",
    "  * Melhor investir em mais camadas do que em mais neurônios por camada.\n",
    "\n",
    "\n",
    "### Taxa de Aprendizado (Learning Rate)\n",
    "\n",
    "* Hiperparâmetro mais crítico.\n",
    "* Estratégia:\n",
    "\n",
    "  * Comece com uma taxa **grande que cause divergência**.\n",
    "  * Divida por 3 até parar de divergir.\n",
    "* Reduzir durante o treinamento pode ajudar (ex: *learning rate schedules*).\n",
    "\n",
    "### Otimizador\n",
    "\n",
    "* Use otimizadores mais avançados que SGD simples, como:\n",
    "\n",
    "  * **Adam**\n",
    "  * **RMSProp**\n",
    "\n",
    "### Tamanho do Lote (Batch Size)\n",
    "\n",
    "* Afeta **desempenho** e **tempo de treinamento**.\n",
    "* Dicas:\n",
    "\n",
    "  * Ideal geralmente < 32\n",
    "  * > 10 permite otimizações de hardware/software\n",
    "  * Com *Batch Normalization*, evite batches < 20\n",
    "\n",
    "\n",
    "### Função de Ativação\n",
    "\n",
    "* **ReLU**: padrão recomendado para camadas ocultas.\n",
    "* A função de ativação da **camada de saída** depende da tarefa:\n",
    "\n",
    "  * Classificação binária → Sigmoid\n",
    "  * Classificação multiclasse → Softmax\n",
    "  * Regressão → Linear\n",
    "\n",
    "\n",
    "### Número de Iterações\n",
    "\n",
    "* Em vez de fixar manualmente, use **Early Stopping**:\n",
    "\n",
    "  * Para o treinamento quando o desempenho em validação para de melhorar."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
